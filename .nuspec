<?xml version="1.0"?>
<package>
  <metadata>
    <id>lmstudio</id>
    <version>0.3.21</version>
    <title>LM Studio Discover, download, and run local LLMs</title>
    <authors>LM Studio</authors>
    <owners>Mohammad Abu Mattar</owners>
    <licenseUrl>https://lmstudio.ai/terms</licenseUrl>
    <projectUrl>https://lmstudio.ai/</projectUrl>
    <iconUrl>https://avatars.githubusercontent.com/u/133744619</iconUrl>
    <requireLicenseAcceptance>false</requireLicenseAcceptance>
    <description><![CDATA[
# LM Studio AI Chocolatey Package

## Downloads

To install LM Studio, run the following command from the command line or from PowerShell:

```powershell
choco install lmstudio
```

This command will download the LM Studio installer from the official website and execute it silently.

## Manual Installation

To install LM Studio manually, download LM Studio from [lmstudio.ai](https://lmstudio.ai/).

## What is LM Studio?

This package automates the installation of the LM Studio application. LM Studio is a desktop application that allows you to run large language models (LLMs) on your laptop, entirely offline.

## Features

With LM Studio, you can:

â€¢ ðŸ¤– Run LLMs on your laptop, entirely offline

â€¢ ðŸ“š Chat with your local documents (new in 0.3)

â€¢ ðŸ‘¾ Use models through the in-app Chat UI or an OpenAI compatible local server

â€¢ ðŸ“‚ Download any compatible model files from Hugging Face ðŸ¤— repositories

â€¢ ðŸ”­ Discover new & noteworthy LLMs right inside the app's Discover page

LM Studio supports any GGUF `Llama 3.2`, `Mistral`, `Phi`, `Gemma`, `DeepSeek`, `Qwen 2.5`, `StarCoder`, etc model on Hugging Face.

## Minimum Requirements

Minimum requirements: M1/M2/M3/M4 Mac, or a Windows / Linux PC with a processor that supports AVX2.

Made possible thanks to the [llama.cpp project](https://github.com/ggerganov/llama.cpp).

## Documentation

Consult the Technical Documentation at [https://lmstudio.ai/docs](https://lmstudio.ai/docs).

## Frequently Asked Questions

### **Does LM Studio collect any data?**

No. One of the main reasons for using a local LLM is privacy, and LM Studio is designed for that. Your data remains private and local to your machine.

See [Documentation > Offline Operation](https://lmstudio.ai/docs/offline) for more.

### **What are the minimum hardware / software requirements?**

Visit the [System Requirements](https://lmstudio.ai/docs/system-requirements) page for the most up to date information.

## Stay Updated

ðŸš€ New in 0.3.21: `reasoning_content` in API responses and Idle TTL! Read the [announcement](https://lmstudio.ai/blog/lmstudio-v0.3.21).
    ]]></description>
    <summary>LM Studio is a desktop application that allows you to run large language models (LLMs) on your laptop, entirely offline.</summary>
    <releaseNotes>https://lmstudio.ai/blog/lmstudio-v0.3.21</releaseNotes>
    <tags>lm-studio ai llm ollama</tags>
    <packageSourceUrl>https://github.com/MKAbuMattar/lm-studio-chocolatey-package</packageSourceUrl>
    <docsUrl>https://lmstudio.ai/docs/app</docsUrl>
  </metadata>
  <files>
    <file src="tools\**" target="tools" />
  </files>
</package>
